
# More inspiration: https://github.com/mcabbott/Tullio.jl

fn image_stuff() {
	# dim() creates a unique object with an optional size property

	let H = dim(); # any size dim, but means "one particular size" |H|
	let W = dim();
	let C = dim(3); # "statically" known size.


	#         | B1 "dim" variable gets created here
	#         v
	let img1[B1=5, H, W, C] <- 0; # <- is "parallel/broadcast/tensor/array assign"

	let img1[H, W1=(B1, W), C] <- img[B1, H, W, C];
	#                ^
	#                | einops-like rearrange (done inside indexing?)

	#         | B2 gets created here
	#         v
	let img2[B2=4, H, W, C] <- random([0, 1]);
	let img2[H, W2=(B2, W), C] <- img2[B, H, W, C]);

	# einops concat/stack extension
	let all_img[H, W1+W2, C] <- img[H, W1, C], img2[H, W2, C];
	
	send display(all_img);

}

# dims in the type signature of a function must be declared before the definition of that function.
let B = dim();
let Seq = dim();
let D = dim();

# Types of dim:
#      dim() : Ordinary dim. Stands in for some concrete value which can be any non-negative integer.
#      dim(5) : Ordinary dim, already initialized to a value.
#      dim(min=1) : Lower-bounded dim. Allows operations that require some data to operate on.
#      dim(min=4) : As above, but probably more rarely used??
# (??) dim(max=10) : Upper-bounded dim.
# 					 Not sure why this would be neccesary?
#      dim.product(dim(), dim(), ...) : A dim that represents the cartesian product / flattening of N other dims.
# 										Order-sensitive due to array packing.
#      dim.sum(dim(), dim(), ...) : A dim that represents the concatenation of N other dims.
# 									Order-sensitive due to packing order.
# (??) dim.list(dim()) : A list of dims. (for use like ...D )
# 						 Could just use lists though, like [Dim]?
# (??) dim.sparse(dim()) : Tell the runtime to use a sparse representation for this dim.
# 						   Definitely a stretch goal.
# (??) dim.ragged()

# transformer attention with Q-per-head but single K and V. Allows
# faster inference - speeds up significantly.
#                                              | Denotes that we are using a variable as an argument name,
# 											   | which means it is guaranteed unique, and can have the type
#                                              | already specified. "shape" is a built-in variable and function,
#                                              | which is implicitly passed in ein assign statements.
#                                              v  
fn multi_query_attention(x: f32[...B, Seq, D], ~shape): f32[...B, Seq, D] {
	
	# create dims (private to this function) that will be shared across multiple vars
	# Dims created within a function can't be in the type of the return value of the function.
	# They have to be created outside the function.
	let KQ = dim();
	let V = dim();
	let H = dim();

	let wq[D, H, QK] <- send get_variable();
	let wk[D, QK] <- send get_variable();
	let wv[D, V] <- send get_variable();
	let wo[H, V, D] <- send get_variable();
	
	# einops-like "contraction"
	let q[...B, Seq, H, QK] <- x[...B, Seq, D], wq[D, H, QK];
	let k[...B, Seq, QK] <- x[...B, Seq, D], wk[D, QK];
	let v_in[...B, Seq, V] <- x[...B, Seq, D], wv[D, V];

	let attn_logits[B, Seq, Seq, H] <- q[...B, Seq, H, QK], k[...B, Seq, QK];
	let attn_weights = softmax(attn_logits);
	
	let v[...B, Seq, H, V] <- attn_weights[...B, Seq, Seq, H], v_in[...B, V];
	
	let v_out[...B, Seq, D] <- v[...B, Seq, H, V], wo[H, V, D];
	
	v_out
}
